{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd1Gq8URmduz5KrPP5Fv46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olegault/CODE-V-WADE/blob/main/PolisisCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuGQuMIwCKaA"
      },
      "outputs": [],
      "source": [
        "#Imports needed from pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD,Adam\n",
        "\n",
        "#Some built-in imports\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from os.path import join, isfile\n",
        "from os import listdir\n",
        "import json\n",
        "\n",
        "# SKLearn and Skorch\n",
        "from sklearn.datasets import make_classification\n",
        "from skorch import NeuralNet, NeuralNetClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.metrics import classification_report\n",
        "from skorch.callbacks import EarlyStopping\n",
        "\n",
        "#Imports from the repository\n",
        "from data_processing import get_weights_matrix, get_tokens\n",
        "import data_processing as dp\n",
        "from privacy_policies_dataset import PrivacyPoliciesDataset as PPD\n",
        "from database import Database\n",
        "\n",
        "from urllib.parse import unquote\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    Convolutional Neural Model used for training the models. The total number of kernels that will be used in this\n",
        "    CNN is Co * len(Ks). \n",
        "    \n",
        "    Args:\n",
        "        weights_matrix: numpy.ndarray, the shape of this n-dimensional array must be (words, dims) were words is\n",
        "        the number of words in the vocabulary and dims is the dimensionality of the word embeddings.\n",
        "        Co (number of filters): integer, stands for channels out and it is the number of kernels of the same size that will be used.\n",
        "        Hu: integer, stands for number of hidden units in the hidden layer.\n",
        "        C: integer, number of units in the last layer (number of classes)\n",
        "        Ks: list, list of integers specifying the size of the kernels to be used. \n",
        "     \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embeddings, vocab_size, emb_dim, Co, Hu, C, Ks, dropout, name = 'generic'):\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "              \n",
        "        self.num_embeddings = vocab_size\n",
        "        \n",
        "        self.embeddings_dim = emb_dim\n",
        "\n",
        "        self.padding_index = 0\n",
        "        \n",
        "        self.cnn_name = 'cnn_' + str(emb_dim) + '_' + str(Co) + '_' + str(Hu) + '_' + str(C) + '_' + str(Ks) + '_' + name\n",
        "\n",
        "        self.Co = Co\n",
        "        \n",
        "        self.Hu = Hu\n",
        "        \n",
        "        self.C = C\n",
        "        \n",
        "        self.Ks = Ks\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embeddings_dim, self.padding_index)\n",
        "        self.embedding = self.embedding.from_pretrained(torch.tensor(embeddings).float(), freeze=True)\n",
        "\n",
        "        self.convolutions = nn.ModuleList([nn.Conv2d(1,self.Co,(k, self.embeddings_dim)) for k in self.Ks])\n",
        "        \n",
        "        # activation function for hidden layers =  Rectified Linear Unit\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.drop_out = nn.Dropout(p=dropout)\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.Co * len(self.Ks), self.Hu[0])\n",
        "        \n",
        "        self.linear2 = nn.Linear(self.Hu[-1], self.C)\n",
        "        \n",
        "        # activation function of output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        self.double()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        \n",
        "        #size(N,1,length) to size(N,1,length,dims)\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        #size(N,1,length,dims) to size(N,1,length)\n",
        "        \n",
        "        x = [self.relu(conv(x)).squeeze(3) for conv in self.convolutions]\n",
        "        \n",
        "        #size(N,1,length) to (N, Co * len(Ks))\n",
        "        \n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        \n",
        "        x = torch.cat(x,1)\n",
        "        \n",
        "        x = self.linear1(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear2(x)\n",
        "\n",
        "        x = self.sigmoid(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "WdoQsopWEKYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_lists(policy_text):\n",
        "    policy_text_filtered_lists = []\n",
        "    for line_index in range(len(policy_text)):\n",
        "        if policy_text[line_index][-1] == ',':\n",
        "            whole_segment = policy_text[line_index].split('*')\n",
        "            avg_len = 0\n",
        "            for list_element in whole_segment:\n",
        "                avg_len += len(list_element.split())\n",
        "            avg_len = avg_len / len(whole_segment)\n",
        "            if (avg_len >= 20):\n",
        "                for list_element in whole_segment:\n",
        "                    policy_text_filtered_lists.append(list_element.strip())\n",
        "            else:\n",
        "                if (len(policy_text_filtered_lists) == 0):\n",
        "                    policy_text_filtered_lists = [policy_text[line_index]]\n",
        "                else:\n",
        "                    policy_text_filtered_lists[-1] += policy_text[line_index]\n",
        "        else:\n",
        "            policy_text_filtered_lists.append(policy_text[line_index]) \n",
        "    return policy_text_filtered_lists"
      ],
      "metadata": {
        "id": "Shl-6XaNEnME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}