{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2aca3d",
   "metadata": {},
   "source": [
    "# Attribute Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "92aa658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports needed from pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD,Adam\n",
    "\n",
    "#Some built-in imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "# SKLearn and Skorch\n",
    "from sklearn.datasets import make_classification\n",
    "from skorch import NeuralNet, NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "#Imports from the repository\n",
    "from data_processing import get_weights_matrix, get_tokens\n",
    "import data_processing as dp\n",
    "from privacy_policies_dataset import PrivacyPoliciesDataset as PPD\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531b961",
   "metadata": {},
   "source": [
    "# 1. Declare Attribute to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "120162ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_attribute = 'Choice Type'\n",
    "current_num_levels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "967819fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Generic', 0),\n",
       "             ('Data access limitation', 1),\n",
       "             ('Privacy review/audit', 2),\n",
       "             ('Privacy training', 3),\n",
       "             ('Privacy/Security program', 4),\n",
       "             ('Secure data storage', 5),\n",
       "             ('Secure data transfer', 6),\n",
       "             ('Secure user authentication', 7),\n",
       "             ('Unspecified', 8)])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.attr_value_labels(current_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5472b",
   "metadata": {},
   "source": [
    "# 2. Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "152eef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(input_path, output_path, dims = 300, read = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    This functions returns two dictionaries that process the fasttext folder and gets the pretrained \n",
    "    embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "        input_path: string, path to the pretrained embeddings\n",
    "        output_path: string, path to save dictionaries extracted from the pretrained embeddings\n",
    "        dims: integer, embeddings dimensionality to use. (Default = 300)\n",
    "        read: boolean, variable that allows us to decide wether to read from pre-processed files or not.\n",
    "    Returns:\n",
    "        word2vector: dictionary, the keys are the words and the values are the embeddings associated with that word.\n",
    "        word2idx: dictionary, the keys are the words and the values are the indexes associated with that word.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def append_from_file(words, word2idx, vectors, idx, input_path):\n",
    "        \n",
    "        with open(input_path, encoding=\"utf8\") as fast_text_file:\n",
    "\n",
    "            for line in fast_text_file:\n",
    "\n",
    "                split_line = line.split()\n",
    "\n",
    "                word = split_line[0]\n",
    "\n",
    "                words.append(word)\n",
    "\n",
    "                word2idx[word] = idx\n",
    "\n",
    "                vector = np.array(split_line[1:]).astype(float)\n",
    "\n",
    "                vectors.append(vector)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return words, word2idx, vectors, idx\n",
    "    \n",
    "\n",
    "    word2vector_path = \"word2vector_\" + str(dims) + \".pkl\"\n",
    "\n",
    "    word2vector_path = join(output_path, word2vector_path)\n",
    "\n",
    "    word2idx_path = \"word2idx_\" + str(dims) + \".pkl\"\n",
    "\n",
    "    word2idx_path = join(output_path, word2idx_path)\n",
    "    \n",
    "    if isfile(word2vector_path) and isfile(word2idx_path) and read:\n",
    "        \n",
    "        print(\"Loading from file {}\".format(word2vector_path))\n",
    "\n",
    "        with open(word2vector_path,\"rb\") as word2vector_file:\n",
    "        \n",
    "            word2vector = pickle.load(word2vector_file)\n",
    "            \n",
    "        print(\"Loading from file {}\".format(word2idx_path))\n",
    "\n",
    "        with open(word2idx_path,\"rb\") as word2idx_file:\n",
    "        \n",
    "            word2idx = pickle.load(word2idx_file)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Processing dataset ...\")\n",
    "\n",
    "        words = [None]\n",
    "\n",
    "        word2idx = {None: 0}\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        vectors = [np.zeros(dims)]\n",
    "        \n",
    "        words, word2idx, vectors, idx = append_from_file(words, word2idx, vectors, idx, input_path)     \n",
    "                           \n",
    "        word2vector = {w: vectors[word2idx[w]] for w in words}\n",
    "        \n",
    "        with open(word2vector_path,\"wb\") as word2vector_file:\n",
    "        \n",
    "            pickle.dump(word2vector, word2vector_file)\n",
    "        \n",
    "        with open(word2idx_path,\"wb\") as word2idx_file:\n",
    "        \n",
    "            pickle.dump(word2idx, word2idx_file)\n",
    "\n",
    "    return word2vector, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b7569f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file ./embeddings/word2vector_300.pkl\n",
      "Loading from file ./embeddings/word2idx_300.pkl\n",
      "Loading from file weights_matrix_300.pkl\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = './embeddings/pretrained_embeddings_300.vec'\n",
    "word2vector, word2idx = get_dicts(pretrained_embeddings, f\"./embeddings/\", 300, read = True)\n",
    "weights_matrix = get_weights_matrix(300, f\"./embeddings/\", oov_random = False, dictionary = word2idx, word2vector = word2vector, read = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0395b",
   "metadata": {},
   "source": [
    "# Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "27accc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Browser/device privacy controls\n",
      "1. Dont use service/feature\n",
      "2. First-party privacy controls\n",
      "3. Opt-in\n",
      "4. Opt-out link\n",
      "5. Opt-out via contacting company\n",
      "6. Third-party privacy controls\n",
      "7. Unspecified\n"
     ]
    }
   ],
   "source": [
    "labels_file = open(f\"labels/labels_{current_attribute}.pkl\",\"rb\")\n",
    "\n",
    "labels = pickle.load(labels_file)\n",
    "\n",
    "labels_file.close()\n",
    "\n",
    "target_names = []\n",
    "label_indices = []\n",
    "\n",
    "for label, index in labels.items():\n",
    "    target_names.append(label)\n",
    "    label_indices.append(index)\n",
    "    print(str(index) + '. ' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c57f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset in one file ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Majority'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11110/577890127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_data_attribute_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_attribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_num_levels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/polisis-replication/data_processing.py\u001b[0m in \u001b[0;36maggregate_data_attribute_level\u001b[0;34m(attribute, num_labels, read)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0maggregate_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maggregate_data_attribute_level_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/polisis-replication/data_processing.py\u001b[0m in \u001b[0;36maggregate_files\u001b[0;34m(input_path, output_path, labels_dict)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'segment'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Majority'"
     ]
    }
   ],
   "source": [
    "dp.aggregate_data_attribute_level(current_attribute, current_num_levels, read = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d317c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from processed_data/all_sentence_matrices_Security Measure.pkl and processed_data/all_label_matrices_Security Measure.pkl\n"
     ]
    }
   ],
   "source": [
    "sentence_matrices_all, labels_matrices_all = dp.process_dataset_attribute_level(labels, word2idx, current_attribute, read = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce7f0d",
   "metadata": {},
   "source": [
    "We now create an PPD which stands for PrivacyPoliciesDataset containing the training and testing dataset. We will need to split the data in two to get the test training data and the data that will be used for training and validation. The function split_dataset_randomly is spliting the dataset 90/10 by default. It uses a consistent random seed as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac0784b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = PPD(sentence_matrices_all, labels_matrices_all, labels)\n",
    "\n",
    "# test_dataset, train_dataset = dataset.split_dataset_randomly(ratio = 0.2)\n",
    "\n",
    "# test_dataset.pickle_dataset(f\"datasets/test_dataset_{current_attribute}.pkl\")\n",
    "\n",
    "# train_dataset.pickle_dataset(f\"datasets/train_dataset_{current_attribute}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "39f730c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"datasets/train_dataset_{current_attribute}.pkl\", \"rb\") as dataset_file:\n",
    "    train_dataset = pickle.load(dataset_file)\n",
    "with open(f\"datasets/test_dataset_{current_attribute}.pkl\", \"rb\") as dataset_file:\n",
    "    test_dataset = pickle.load(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b7176973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments: 2306\n",
      "Num of labels: 3828\n",
      "Percentages with respect to number of labels ... \n",
      "0. Browser/device privacy controls : 382 (9.979101358411704%)\n",
      "1. Dont use service/feature : 570 (14.890282131661442%)\n",
      "2. First-party privacy controls : 308 (8.045977011494253%)\n",
      "3. Opt-in : 947 (24.73876698014629%)\n",
      "4. Opt-out link : 549 (14.341692789968652%)\n",
      "5. Opt-out via contacting company : 383 (10.005224660397074%)\n",
      "6. Third-party privacy controls : 257 (6.713688610240334%)\n",
      "7. Unspecified : 432 (11.285266457680251%)\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Num of segments: 576\n",
      "Num of labels: 977\n",
      "Percentages with respect to number of labels ... \n",
      "0. Browser/device privacy controls : 81 (8.290685772773797%)\n",
      "1. Dont use service/feature : 145 (14.841351074718526%)\n",
      "2. First-party privacy controls : 71 (7.267144319344934%)\n",
      "3. Opt-in : 264 (27.021494370522007%)\n",
      "4. Opt-out link : 135 (13.817809621289662%)\n",
      "5. Opt-out via contacting company : 98 (10.030706243602866%)\n",
      "6. Third-party privacy controls : 71 (7.267144319344934%)\n",
      "7. Unspecified : 112 (11.463664278403275%)\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset.labels_stats()\n",
    "print(\"-\" * 35 * 3)\n",
    "test_dataset.labels_stats()\n",
    "print(\"-\" * 35 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8fe94",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05d65adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Convolutional Neural Model used for training the models. The total number of kernels that will be used in this\n",
    "    CNN is Co * len(Ks). \n",
    "    \n",
    "    Args:\n",
    "        weights_matrix: numpy.ndarray, the shape of this n-dimensional array must be (words, dims) were words is\n",
    "        the number of words in the vocabulary and dims is the dimensionality of the word embeddings.\n",
    "        Co (number of filters): integer, stands for channels out and it is the number of kernels of the same size that will be used.\n",
    "        Hu: integer, stands for number of hidden units in the hidden layer.\n",
    "        C: integer, number of units in the last layer (number of classes)\n",
    "        Ks: list, list of integers specifying the size of the kernels to be used. \n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, vocab_size, emb_dim, Co, Hu, C, Ks, dropout, name = 'generic'):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "              \n",
    "        self.num_embeddings = vocab_size\n",
    "        \n",
    "        self.embeddings_dim = emb_dim\n",
    "\n",
    "        self.padding_index = 0\n",
    "        \n",
    "        self.cnn_name = 'cnn_' + str(emb_dim) + '_' + str(Co) + '_' + str(Hu) + '_' + str(C) + '_' + str(Ks) + '_' + name\n",
    "\n",
    "        self.Co = Co\n",
    "        \n",
    "        self.Hu = Hu\n",
    "        \n",
    "        self.C = C\n",
    "        \n",
    "        self.Ks = Ks\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embeddings_dim, self.padding_index)\n",
    "        self.embedding = self.embedding.from_pretrained(torch.tensor(embeddings).float(), freeze=True)\n",
    "\n",
    "        self.convolutions = nn.ModuleList([nn.Conv2d(1,self.Co,(k, self.embeddings_dim)) for k in self.Ks])\n",
    "        \n",
    "        # activation function for hidden layers =  Rectified Linear Unit\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.Co * len(self.Ks), self.Hu[0])\n",
    "        \n",
    "        self.linear2 = nn.Linear(self.Hu[-1], self.C)\n",
    "        \n",
    "        # activation function of output layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.double()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #size(N,1,length) to size(N,1,length,dims)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #size(N,1,length,dims) to size(N,1,length)\n",
    "        \n",
    "        x = [self.relu(conv(x)).squeeze(3) for conv in self.convolutions]\n",
    "        \n",
    "        #size(N,1,length) to (N, Co * len(Ks))\n",
    "        \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        \n",
    "        x = torch.cat(x,1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e738ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(batch):\n",
    "\n",
    "    def stack_segments(segments, clearance = 2):\n",
    "\n",
    "        import numpy as np\n",
    "\n",
    "        segments_len = map(len, segments)\n",
    "        max_len = max(segments_len)\n",
    "\n",
    "        segments_list = []\n",
    "\n",
    "        output_len = max_len + clearance * 2\n",
    "\n",
    "        for i, segment in enumerate(segments):\n",
    "\n",
    "            segment_array = np.array(segment)\n",
    "\n",
    "            zeros_to_prepend = int((output_len - len(segment_array))/2)\n",
    "\n",
    "            zeros_to_append = output_len - len(segment_array) - zeros_to_prepend\n",
    "\n",
    "            resized_array = np.append(np.zeros(zeros_to_prepend), segment_array)\n",
    "\n",
    "            resized_array = np.append(resized_array, np.zeros(zeros_to_append))\n",
    "\n",
    "            segments_list.append(torch.tensor(resized_array, dtype = torch.int64, device=torch.device(\"cuda\")))\n",
    "\n",
    "            segments_tensor = torch.stack(segments_list).unsqueeze(1)\n",
    "\n",
    "        return segments_tensor                         \n",
    "\n",
    "    segments = [item[0] for item in batch]\n",
    "\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    segments_tensor = stack_segments(segments)\n",
    "\n",
    "    labels_tensor = torch.stack(labels)\n",
    "\n",
    "    return [segments_tensor, labels_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1e1a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_f1_presence(y_true, y_pred):\n",
    "    y_pred = y_pred > 0.5\n",
    "    return f1_score(y_true, y_pred, average='macro', zero_division='warn')\n",
    "\n",
    "def my_custom_f1_absence(y_true, y_pred):\n",
    "    y_pred = y_pred <= 0.5\n",
    "    return f1_score(y_true < 1, y_pred, average='macro', zero_division='warn')\n",
    "\n",
    "\n",
    "score_presence = make_scorer(my_custom_f1_presence, needs_proba=True)\n",
    "score_absence = make_scorer(my_custom_f1_absence, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "af76095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    CNN,\n",
    "    module__embeddings = weights_matrix,\n",
    "    module__vocab_size = weights_matrix.shape[0],\n",
    "    module__emb_dim = weights_matrix.shape[1],\n",
    "    module__Co = 200,\n",
    "    module__Hu = [100],\n",
    "    module__C = current_num_levels,\n",
    "    module__Ks = [3],\n",
    "    module__name = f'{current_attribute}_zeros_60-20-(no-val)_polisis',\n",
    "    module__dropout = 0.5,\n",
    "    max_epochs = 100,\n",
    "    lr = 0.001,\n",
    "    optimizer = SGD,\n",
    "    optimizer__weight_decay = 0,\n",
    "    optimizer__momentum=0.9,\n",
    "    criterion = nn.BCELoss(),\n",
    "    batch_size=40,\n",
    "    # Turn the validation split off once we have the metadata values set\n",
    "    train_split = None,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn=collate_data,\n",
    "    iterator_valid__collate_fn=collate_data,\n",
    "    # Turn off verbose\n",
    "#     verbose = 0,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a579038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.fit(train_dataset.segments_array, train_dataset.labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0650845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.01, 0.001],\n",
    "#     'max_epochs': [75, 100, 200, 300]\n",
    "# }\n",
    "# gs = RandomizedSearchCV(net, params, refit='presence', cv=5,  scoring={'presence': score_presence,'absence': score_absence})\n",
    "# gs.fit(train_dataset.segments_array, train_dataset.labels_tensor)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs.best_estimator_.save_params(f_params=f'trained_models/{current_attribute}/model.pkl',f_optimizer=f'trained_models/{current_attribute}/optimizer.pkl', f_history=f'trained_models/{current_attribute}/history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa3c2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.save_params(f_params=f'trained_models/{current_attribute}/model.pkl',f_optimizer=f'trained_models/{current_attribute}/optimizer.pkl', f_history=f'trained_models/{current_attribute}/history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff0cce",
   "metadata": {},
   "source": [
    "# Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "577654da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "net = NeuralNet(\n",
    "    CNN,\n",
    "    module__embeddings = weights_matrix,\n",
    "    module__vocab_size = weights_matrix.shape[0],\n",
    "    module__emb_dim = weights_matrix.shape[1],\n",
    "    module__Co = 200,\n",
    "    module__Hu = [100],\n",
    "    module__C = current_num_levels,\n",
    "    module__Ks = [3],\n",
    "    module__name = f'{current_attribute}_zeros_60-20-(no-val)_polisis',\n",
    "    module__dropout = 0.5,\n",
    "    max_epochs = 300,\n",
    "    lr = 0.01,\n",
    "    optimizer = SGD,\n",
    "    optimizer__weight_decay = 0,\n",
    "    optimizer__momentum=0.9,\n",
    "    criterion = nn.BCELoss(),\n",
    "    batch_size=40,\n",
    "    # Turn the validation split off once we have the metadata values set\n",
    "    train_split = None,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn=collate_data,\n",
    "    iterator_valid__collate_fn=collate_data,\n",
    "    # Turn off verbose\n",
    "    verbose = 0,\n",
    "    device='cuda',\n",
    ").initialize()\n",
    "net.load_params(f_params=f'trained_models/{current_attribute}/model.pkl',f_optimizer=f'trained_models/{current_attribute}/optimizer.pkl', f_history=f'trained_models/{current_attribute}/history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c7eb2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = net.predict_proba(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "87ad1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "Browser/device privacy controls       0.91      0.88      0.89        81\n",
      "       Dont use service/feature       0.66      0.64      0.65       145\n",
      "   First-party privacy controls       0.77      0.58      0.66        71\n",
      "                         Opt-in       0.85      0.73      0.78       264\n",
      "                   Opt-out link       0.86      0.73      0.79       135\n",
      " Opt-out via contacting company       0.87      0.68      0.77        98\n",
      "   Third-party privacy controls       0.83      0.61      0.70        71\n",
      "                    Unspecified       0.75      0.44      0.55       112\n",
      "\n",
      "                      micro avg       0.81      0.67      0.73       977\n",
      "                      macro avg       0.81      0.66      0.72       977\n",
      "                   weighted avg       0.81      0.67      0.73       977\n",
      "                    samples avg       0.76      0.71      0.70       977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Presence\n",
    "print(classification_report(test_dataset.labels_tensor > 0, y_proba > 0.5, labels=label_indices, target_names=target_names, zero_division='warn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9bd3d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "Browser/device privacy controls       0.98      0.99      0.98       495\n",
      "       Dont use service/feature       0.88      0.89      0.88       431\n",
      "   First-party privacy controls       0.94      0.98      0.96       505\n",
      "                         Opt-in       0.79      0.89      0.84       312\n",
      "                   Opt-out link       0.92      0.96      0.94       441\n",
      " Opt-out via contacting company       0.94      0.98      0.96       478\n",
      "   Third-party privacy controls       0.95      0.98      0.96       505\n",
      "                    Unspecified       0.88      0.97      0.92       464\n",
      "\n",
      "                      micro avg       0.92      0.96      0.94      3631\n",
      "                      macro avg       0.91      0.95      0.93      3631\n",
      "                   weighted avg       0.92      0.96      0.94      3631\n",
      "                    samples avg       0.92      0.96      0.93      3631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Absence\n",
    "print(classification_report(test_dataset.labels_tensor < 1, y_proba <= 0.5, labels=label_indices, target_names=target_names, zero_division='warn'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
